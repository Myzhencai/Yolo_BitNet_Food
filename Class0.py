import os
from PIL import Image
from ultralytics import YOLO
from collections import Counter
import os
import sys
import signal
import platform
import subprocess
from google import genai

# åˆå§‹åŒ–Google GenAIå®¢æˆ·ç«¯
client = genai.Client(api_key="AIzaSyDnJ-HAfTYa7hdO4V2xXhuIAbElsjfxtSI")



def run_command(command, shell=False):
    """Run a system command and ensure it succeeds."""
    try:
        subprocess.run(command, shell=shell, check=True)
    except subprocess.CalledProcessError as e:
        print(f"Error occurred while running command: {e}")
        sys.exit(1)

def run_inference(promptsvalue):
    # è®¾å®šé»˜è®¤å‚æ•°
    model = "D:\\JudyYolo\\angent\\BitNet\\models\\BitNet-b1.58-2B-4T\\ggml-model-i2_s.gguf"
    n_predict = 60000
    prompt = promptsvalue
    threads = 6
    ctx_size = 2048
    temperature = 0.8
    conversation = True

    build_dir = "D:\\JudyYolo\\angent\\BitNet\\build"
    if platform.system() == "Windows":
        main_path = os.path.join(build_dir, "bin", "Release", "llama-cli.exe")
        if not os.path.exists(main_path):
            main_path = os.path.join(build_dir, "bin", "llama-cli")
    else:
        main_path = os.path.join(build_dir, "bin", "llama-cli")

    command = [
        f'{main_path}',
        '-m', model,
        '-n', str(n_predict),
        '-t', str(threads),
        '-p', prompt,
        '-ngl', '0',
        '-c', str(ctx_size),
        '--temp', str(temperature),
        "-b", "1",
    ]
    if conversation:
        command.append("-cnv")

    run_command(command)

def signal_handler(sig, frame):
    print("Ctrl+C pressed, exiting...")
    sys.exit(0)



def load_model(model_path, device='cpu'):
    model = YOLO(model_path)
    model.to(device)
    print(f"Model loaded to {device}")
    return model

def predict_and_save(model, img_path, conf=0.25, save_path="result.jpg"):
    results = model.predict(source=img_path, conf=conf, save=False)
    result = results[0]

    # ç»Ÿè®¡è¯†åˆ«ç»“æœ
    class_counts = Counter(result.boxes.cls.numpy().astype(int))

    # æ„å»ºæè¿°å­—ç¬¦ä¸²
    parts = []
    for cls_id, count in class_counts.items():
        cls_name = result.names[cls_id]
        parts.append(f"{count} {cls_name}")

    summary = "I have " + " and ".join(parts) + "."
    print(summary)

    return summary

def predict_and_save2(model, img_path, conf=0.25, save_path="result.jpg"):
    # æ¨¡å‹é¢„æµ‹
    results = model.predict(source=img_path, conf=conf, save=False)
    result = results[0]

    # è·å–é¢„æµ‹æ¡†ç›¸å…³ä¿¡æ¯
    boxes = result.boxes.xyxy.cpu().numpy()       # é¢„æµ‹æ¡†çš„åæ ‡ [x1, y1, x2, y2]
    classes = result.boxes.cls.cpu().numpy().astype(int)  # ç±»åˆ«ç´¢å¼•
    scores = result.boxes.conf.cpu().numpy()      # ç½®ä¿¡åº¦

    # ç»Ÿè®¡è¯†åˆ«ç»“æœ
    class_counts = Counter(classes)

    # æ„å»ºæè¿°å­—ç¬¦ä¸²
    parts = []
    for cls_id, count in class_counts.items():
        cls_name = result.names[cls_id]
        parts.append(f"{count} {cls_name}")
    summary = "I have " + " and ".join(parts) + "."
    print(summary)

    # åœ¨å›¾åƒä¸Šç»˜åˆ¶é¢„æµ‹ç»“æœ
    im_array = result.plot()
    im = Image.fromarray(im_array[..., ::-1])  # BGR to RGB
    im.save(save_path)
    print(f"Saved annotated image to {save_path}")

    return summary


if __name__ == "__main__":
    # æ¨¡å‹è·¯å¾„å’Œå›¾ç‰‡è·¯å¾„
    cfg_model_path = 'models/best 908.pt'
    image_path = 'image/upload.jpg'  # ä¿®æ”¹ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„
    # image_path = 'image/1.png'  # ä¿®æ”¹ä¸ºä½ çš„å›¾ç‰‡è·¯å¾„
    output_path = 'image/output.jpg'
    confidence = 0.25

    if not os.path.exists(cfg_model_path):
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        exit(1)
    if not os.path.exists(image_path):
        print("âŒ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        exit(1)

    model = load_model(cfg_model_path, device='cpu')
    summaryprompt = predict_and_save2(model, image_path, confidence, output_path)

    # signal.signal(signal.SIGINT, signal_handler)
    # promptsvalue = summaryprompt + " can you suggest me a dish ?"
    # run_inference(promptsvalue)
    promptdemo = """
    ææ–™å‡†å¤‡
    ğŸ¥” ä¸»æ–™
    åœŸè±† 2 ä¸ªï¼ˆçº¦ 400 å…‹ï¼‰
    ä¸‹å¨æˆ¿

    é’æ¤’/å°–æ¤’ 2â€“3 ä¸ªï¼ˆçº¦ 100 å…‹ï¼‰
    ä¸‹å¨æˆ¿

    ğŸ¥© å¯é€‰è¾…æ–™
    è‚‰ä¸ 50 å…‹ï¼ˆä¸æ”¾è‚‰ä¹Ÿå¯ï¼Œå…¨ç´ ç‰ˆæœ¬ï¼‰
    ä¸‹å¨æˆ¿

    ğŸŒ± è°ƒæ–™
    é£Ÿç”¨æ²¹ é€‚é‡
    è‘±ã€è’œ å„ 1 å°æŠŠï¼ˆåˆ‡æœ«ï¼‰
    ä¸‹å¨æˆ¿
    æ–™é…’ 1 èŒ¶å‹º
    ç”ŸæŠ½ 1 å¤§å‹º
    ç™½ç³– Â½ èŒ¶å‹ºï¼ˆå¯é€‰ï¼‰
    ç› é€‚é‡

    æ“ä½œæ­¥éª¤
    ğŸ”ª åˆ‡ä¸æ³¡æ°´
    åœŸè±†å»çš®åˆ‡ç»†ä¸ï¼Œæ”¾å…¥æ¸…æ°´ä¸­æµ¸æ³¡ 5 åˆ†é’Ÿä»¥å»é™¤å¤šä½™æ·€ç²‰ï¼Œæ°´æ¸…åæå‡ºæ²¥å¹²
    ä¸‹å¨æˆ¿

    è¾£æ¤’å»è’‚åˆ‡ä¸ï¼Œè‘±è’œåˆ‡æœ«å¤‡ç”¨

    ğŸ³ è¿‡æ²¹æˆ–å¹²ç‚’
    é”…ä¸­å€’å…¥é€‚é‡æ²¹ï¼Œçƒ§è‡³äº”æˆçƒ­ï¼ˆçº¦ 150â„ƒï¼‰
    ä¸‹å…¥åœŸè±†ä¸å¤§ç«å¿«é€Ÿç¿»ç‚’ 1â€“2 åˆ†é’Ÿï¼Œè‡³åœŸè±†â€œæ–­ç”Ÿâ€å³å¯æå‡ºæ§æ²¹
    æœç‹

    ğŸ³ çˆ†é¦™è°ƒå‘³
    é”…ç•™åº•æ²¹ï¼Œå…ˆä¸‹è‘±æœ«ã€è’œæœ«çˆ†é¦™çº¦ 10 ç§’
    å€’å…¥è‚‰ä¸ç¿»ç‚’è‡³å˜è‰²ï¼ŒåŠ å…¥æ–™é…’ã€ç”ŸæŠ½å¿«é€Ÿç¿»ç‚’å‡åŒ€

    ğŸ”„ åˆç‚’æ”¶æ±
    å°†æ§å¹²æ°´çš„åœŸè±†ä¸å€’å›é”…ä¸­ï¼Œå¤§ç«ç¿»ç‚’è‡³å…¥å‘³
    åŠ å…¥è¾£æ¤’ä¸ç»§ç»­ç¿»ç‚’ 30 ç§’
    æ’’å…¥ç›ã€ç™½ç³–ï¼Œå†ç¿»ç‚’ 10â€“15 ç§’å³å¯å‡ºé”…
    ä¸‹å¨æˆ¿

    å°è´´å£«
    ğŸ’§ æ§æ°´è¦å……åˆ†ï¼šåœŸè±†ä¸åˆ‡å¥½ååŠ¡å¿…æ§å‡€è¡¨é¢æ°´åˆ†ï¼Œå¦åˆ™æ˜“â€œçˆ†æ²¹â€æˆ–å‡ºæ°´â€‹
    m.meishiq.com

    ğŸ”¥ ç«å€™è¦è¶³ï¼šæ•´ä¸ªè¿‡ç¨‹å‡ç”¨å¤§ç«å¿«ç‚’ï¼Œæ‰èƒ½ä¿æŒåœŸè±†è„†çˆ½å£æ„Ÿâ€‹
    m.meishiq.com

    ğŸ•’ æ—¶é—´è¦çŸ­ï¼šåœŸè±†ä¸å’Œè¾£æ¤’éƒ½ä¸å®œä¹…ç‚’ï¼Œä¿æŒçˆ½è„†æœ€ä½³â€‹
    ä¸‹å¨æˆ¿

    ğŸ¥„ è°ƒæ–™éšå–œå¥½æ·»åŠ ï¼šå¯æ ¹æ®å£å‘³å¢å‡ç³–ã€é†‹æˆ–è¾£æ¤’ç§ç±»

    é€šè¿‡ä»¥ä¸Šç®€å•å‡ æ­¥ï¼Œæ‚¨å°±èƒ½åœ¨å®¶å¿«é€Ÿåšå‡ºè¿™é“ä¸‹é¥­åˆå¼€èƒƒçš„åœŸè±†ç‚’è¾£æ¤’ã€‚äº«ç”¨æ—¶å¯æ­é…ç±³é¥­æˆ–é¦’å¤´ï¼Œä¸€åŒå“å°å®¶å¸¸çš„é­…åŠ›ï¼
    """

    # æ„é€ æ–°çš„å†…å®¹
    # prompt = f"{summaryprompt} Can you suggest a dish for me? ç”¨ä¸­æ–‡å›ç­”å¹¶å‚è€ƒ{promptdemo}å†…å®¹æ ¼å¼ç”Ÿæˆç»“æœæ³¨æ„ä¿ç•™å°å›¾æ ‡"
    prompt = f"{summaryprompt} Can you suggest a dish for me? ç”¨ä¸­æ–‡ç­”å¤ï¼Œè¯·åœ¨å›ç­”è¿‡ç¨‹ä¸­ä½¿ç”¨ç±»ä¼¼äºğŸ”ªè¿™äº›å¨æˆ¿ç±»çš„å°å›¾æ ‡ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æ·»åŠ å¯ä»¥å‚ç…§çš„æ ¼å¼æ˜¯:{promptdemo}"

    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=prompt,
    )
    print(response.text)  # è¾“å‡ºç”Ÿæˆçš„å†…å®¹
